* TODO create a cookiecutten based off of existing spiders
**  TODO how to handle the chchch directory config across machines?
   #+begin_src python

     CHCHCH_DIR     = '/home/vanangamudi/agam/projects/code/tamilnlp/indraya-kiruvam/'

     DATA_DIR       = '{}/data/'.format(CHCHCH_DIR)
     DELTAFETCH_DIR = '{}/deltafetch/{{cookiecutter.directory_name}}'.format(CHCHCH_DIR)
     HTTPCACHE_DIR  = '{}/httpcache/{{cookiecutter.directory_name}}'.format(CHCHCH_DIR)

   #+end_src
* tips and tricks
** scrapy seems to load loads all files under its project directory.
   #+begin_src python 
     articles   = response.xpath('//section[contains(@class, "pt-main-news-section")]')
     for article in artciles:
	 item = Item()
	 item['url']        = response.url                                                      
	 .
	 .
	 .
	 author_date = artcile.xpath('//div[@class=article-author]')
	 item['author'] = author_date.xpath('//a/text()').extract()
        

	 item['date']    = selector.xpath(
	     '//span[contains(@class,"pull-right")]/text()'
	 ).extract()[0].strip()

   #+end_src
   
   #+begin_src bash
			 vanangamudi@karunthulai:~/agam/projects/code/tamilnlp/indraya-kiruvam/chchch/ch/puthiyathalaimurai/puthiyathalaimurai/spiders

       $ tree ../
       ../
       ├── __init__.py
       ├── items.py
       ├── middlewares.py
       ├── pipelines.py
       ├── settings.py
       └── spiders
	   ├── feed.json
	   ├── __init__.py
	   ├── puthiyathalaimurai.py
	   └── temp.py

       1 directory, 9 files

       $ scrapy shell http://www.puthiyathalaimurai.com/newsview/105826/Sanjay-Manjrekar-Five-Controversial-Comments-That-Have-Caused-A-Stir-On-Social-Media
       Traceback (most recent call last):
       File "/home/vanangamudi/agam/env/chchch/bin/scrapy", line 8, in <module>
       sys.exit(execute())
       File "/home/vanangamudi/agam/env/chchch/lib/python3.6/site-packages/scrapy/cmdline.py", line 144, in execute
       cmd.crawler_process = CrawlerProcess(settings)
       File "/home/vanangamudi/agam/env/chchch/lib/python3.6/site-packages/scrapy/crawler.py", line 280, in __init__
       super().__init__(settings)
       File "/home/vanangamudi/agam/env/chchch/lib/python3.6/site-packages/scrapy/crawler.py", line 152, in __init__
       self.spider_loader = self._get_spider_loader(settings)
       File "/home/vanangamudi/agam/env/chchch/lib/python3.6/site-packages/scrapy/crawler.py", line 146, in _get_spider_loader
       return loader_cls.from_settings(settings.frozencopy())
       File "/home/vanangamudi/agam/env/chchch/lib/python3.6/site-packages/scrapy/spiderloader.py", line 67, in from_settings
       return cls(settings)
       File "/home/vanangamudi/agam/env/chchch/lib/python3.6/site-packages/scrapy/spiderloader.py", line 24, in __init__
       self._load_all_spiders()
       File "/home/vanangamudi/agam/env/chchch/lib/python3.6/site-packages/scrapy/spiderloader.py", line 51, in _load_all_spiders
       for module in walk_modules(name):
     File "/home/vanangamudi/agam/env/chchch/lib/python3.6/site-packages/scrapy/utils/misc.py", line 88, in walk_modules
     submod = import_module(fullpath)
     File "/usr/lib/python3.6/importlib/__init__.py", line 126, in import_module
     return _bootstrap._gcd_import(name[level:], package, level)
     File "<frozen importlib._bootstrap>", line 994, in _gcd_import
     File "<frozen importlib._bootstrap>", line 971, in _find_and_load
     File "<frozen importlib._bootstrap>", line 955, in _find_and_load_unlocked
     File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
     File "<frozen importlib._bootstrap_external>", line 674, in exec_module
     File "<frozen importlib._bootstrap_external>", line 781, in get_code
     File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
     File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
     File "/home/vanangamudi/agam/projects/code/tamilnlp/indraya-kiruvam/chchch/ch/puthiyathalaimurai/puthiyathalaimurai/spiders/temp.py", line 6
     .
     ^
     SyntaxError: invalid syntax
     (chchch) vanangamudi@karunthulai:~/agam/projects/cod
   #+end_src
** mongodb restart failed
   #+begin_src bash
     sudo rm -rf /tmp/mongodb-27017.sock
     sudo systemctl restart  mongod
     sudo systemctl start  mongod
     sudo systemctl status  mongod
   #+end_src
** xpath expresssions
   #+begin_src python

     links = [
	 urllib.parse.unquote(i)  for i in response.xpath(
	     '//*[@href][not(contains(@href,"javascript:void"))]'
	 ).extract()
     ]


   #+end_src
* scrapy setup
** virutal env
   #+begin_src bash

     virtualenv -p python3 ~/env/chchch

   #+end_src
** setup scrapy
   #+begin_src bash

     sudo apt-get install libdb-dev #for deltafetch
     pip install bs4 scrapy scrapy-deltafetch

   #+end_src
** pymongo
   
   #+begin_src bash

     sudo apt-get install gnupg wget
     wget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add -
     echo "deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/4.4 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.4.list
     sudo apt-get update
     sudo apt-get install -y mongodb

     pip install pymongo

   #+end_src
* TODO use different mongo collections for each crawler
* TODO create a cookiecutter for scrapy
  to use same mongo-pipeline and item class basically
* PROJ Separate projects for separate sites

** DONE hierarchy is as follows
#+begin_src org
  ,* root
  ,** ch :githuborg:
  ,*** ch-hindutamil
  ,*** ch-nakkheeran
  ,*** ch-vikatan
  ,*** and so on
  ,** inaippu-churappi
     to mine links to help with figuring out url patterns for the spiders
  ,** kiruvam-eeni
  ,*** kiruvam.py 
      - create ngrams for individual documents
      - mongodb/plaintext - storage details should be abstracted

  ,*** kiruvam-compose.py
      - compose individual doc ngrams into groups based on give key like author, date, publisher, source such a blog, news etc

  ,** interface to download the corpus

  ,*** queryable via keys
#+end_src

* the initial idea didnt' work for a lot of reasons

** CANC Single spider with xpath config 
   having single spider with config file containing xpaths for items to be extracted is a bad idea for the same reason.
** DONE Single scrapy project for all sites
   having all crawlers under same scrapy project is a mess. it is better to have different scrapy project if the items and pipelines are gonna be different
** DONE switched to mongodb instead of plaintext storage
*** inode exhausted on 50GB machine
    - httpcache, jobsdir for scrapy exhausted inode for 50GB on the scraper gcp machine
    - even though the actual output dumped by scraper wasn't occupying a lot of inodes, this is clearly not scalable for multiple projects because,
    - this is just for running one crawler for hindutamil
   
* hindutamil scraper

  
** move to mongodb
   - httpcache and jobsdir take a lot of space
   - doesn't seem cost effective to run on GCE
   - move move to mongodb
   - monogo db for storage instead of file;
     - inode limit is easily hit when storage is < 50Gb
     - versioning raw text seems overkill, so only ngrams is in plan

     - return statements in makedate() and process_item() is very important
     - for storing in mongodb collections - something with serialization

   Note: line =echo "deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/4.4 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.4.list= needs to be changed based on the linux distro.

   #+begin_src bash
     sudo apt-get install gnupg wget
     wget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add -
     echo "deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/4.4 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.4.list
     sudo apt-get update
     sudo apt-get install -y mongodb-org

   #+end_src


   - enable DBM based caching
   #+begin_src python
     HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.DbmCacheStorage'
   #+end_src

** DATA_DIR and other configs in settings.py

   - added DATA_DIR in settings.py file so that it is accessible to spider objects
   - enable Filesystem based on HTTPCACHE
   - increase reactor threadpool size to allow to allow concurrent DNS resolution apparently[1]
   - install and enable deltafetch middleware

   #+begin_src python
     HTTPCACHE_IGNORE_HTTP_CODES = []
     HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'

     CHCHCH_DIR = '/home/vanangamudi/.chchch'

     DATA_DIR = '{}/data/'.format(CHCHCH_DIR)

     # run like  -- 
     # $ scrapy crawl hindutamil \
     #        -s JOBDIR='CHCHCH_DIR/jobs/hindutamil
     DEPTH_PRIORITY = 1 
     SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleFifoDiskQueue'
     SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.FifoMemoryQueue'

     #https://stackoverflow.com/a/39173768
     """
     Currently Scrapy does DNS resolution in a blocking way with usage 
     of thread pool. With higher concurrency levels the crawling could 
     be slow or even fail hitting DNS resolver timeouts. Possible solution
     to increase the number of threads handling DNS queries. The DNS queue 
     will be processed faster speeding up establishing of connection 
     and crawling overall.
     """
     REACTOR_THREADPOOL_MAXSIZE = 20


     #deltafetch
     # install libdb-dev
     # $ pip install scrapy-deltafetch
     # $ scrapy crawl example -a deltafetch_reset=1
     SPIDER_MIDDLEWARES['scrapy_deltafetch.DeltaFetch'] = 100
     DELTAFETCH_ENABLED = True

     DELTAFETCH_DIR = '{}/deltafetch/hindutamil'.format(CHCHCH_DIR)

   #+end_src

   [1] https://stackoverflow.com/a/39173768

** DONE initial spider 
   - for =http://hindutamil.in= site
   - based off of scrapy.CrawlerSpider
   - override parse() of spider class
     - didn't know that it was not supposed to overridden
     - but for some reason it worked, it still works
   - with files dumped to individual text files via pipeline not the feed dump

* Initial idea
  
** Keep three different repositories

*** one for crawler scripts - contains all crawlers one for each website

*** one for raw text dump from crawlers - versioned by git

*** one for the kiruvam - where the kiruvameenu.sh will dump ngrams 

** the following is the rough setup 

#+begin_src bash
  CRAWLER_SCRIPTS=newspaper_crawler_scripts
  PACHAI_THARAVUTH_THOGUPPU=pachai-tharavuth-thoguppu
  INDRAYA_KIRUVAM=indraya-kiruvam

  LANGUAGE=tamil

  for crawler in $CRAWLER_SCRIPTS/$LANGUAGE/*scraper.py:
  do
      echo 'starting crawler $crawler'
      python3 crawler &
  done


#+end_src

